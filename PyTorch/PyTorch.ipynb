{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d84a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcbaad1",
   "metadata": {},
   "source": [
    "#  Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0f909241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2592, 0.2591],\n",
      "        [0.0296, 0.4160]])\n",
      "tensor([[0.2592, 0.2591],\n",
      "        [0.0296, 0.4160]])\n",
      "tensor([[0.0672, 0.0671],\n",
      "        [0.0009, 0.1731]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "\n",
    "x = torch.mul(x,y)\n",
    "y = x.mul_(y)\n",
    "z = x * y\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c83e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0942, 0.7946, 0.3245, 0.6256],\n",
      "        [0.1589, 0.1302, 0.2667, 0.0672],\n",
      "        [0.2486, 0.2328, 0.8652, 0.9747],\n",
      "        [0.1063, 0.3693, 0.7558, 0.8764]])\n",
      "tensor([[0.0942, 0.7946, 0.3245, 0.6256, 0.1589, 0.1302, 0.2667, 0.0672],\n",
      "        [0.2486, 0.2328, 0.8652, 0.9747, 0.1063, 0.3693, 0.7558, 0.8764]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "y = x.view(-1,8)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c699899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'builtin_function_or_method'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy\n",
    "print(type(b))\n",
    "\n",
    "# Other way around\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a) #, dtype)\n",
    "\n",
    "# Changing one of these will modify both because they are stored in the same memory location (CPU)\n",
    "# Unless you have CUDA enabled\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones\n",
    "    y = y.to(device)\n",
    "    z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe00a58",
   "metadata": {},
   "source": [
    "# Autograd <img src=\"Autograd.png\" style=\"width:110px;height=90px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "700c343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3603,  0.7865,  0.5891], requires_grad=True)\n",
      "tensor([1.6397, 2.7865, 2.5891], grad_fn=<AddBackward0>)\n",
      "tensor([ 5.3773, 15.5295, 13.4065], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "555448bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.6665, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "153c7b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.8804, 2.9830, 3.0102])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # dz/dx\n",
    "print(x.grad) # only works for scalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1361be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.5588e-01, 1.1146e+01, 1.0356e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "z.backward(v) # need argument if it's not a scalar\n",
    "print(x.grad) # only works for scalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0f0c6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6397, 2.7865, 2.5891])\n"
     ]
    }
   ],
   "source": [
    "# how to prevent tracking the gradient\n",
    "x.requires_grad_(False)\n",
    "\n",
    "x.detach()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b2ef0003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "# whenever the backward function is called, the graident for the tensor will be accumulated to the .grad attribute\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)\n",
    "    \n",
    "    weights.grad.zero_() # empty the gradients before next step\n",
    "    \n",
    "optimizer = torch.optim.SGD(weights, lr=0.01) # for gradient descent\n",
    "#optimizer.step()\n",
    "#optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831eabae",
   "metadata": {},
   "source": [
    "# Backpropagation (Know Chain Rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "beb318ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain rule (dz/dx = dz/dy * dy/dx)\n",
    "\n",
    "## Local Gradients (dz/dx = (dx*y)/dx = y)           (dz/dy = (dx*y)/dy = x)\n",
    "\n",
    "## 1) Forward Pass: Compute Loss    2) Compute Local Gradients    3) Backward Pass: Compute dLoss/ dWeights using the Chain Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13968703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "### update weights\n",
    "### next forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c6c44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
